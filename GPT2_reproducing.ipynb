{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xxrJolld2pkE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import seaborn\n",
        "import inspect\n",
        "import torch.nn as nn\n",
        "from dataclasses import  dataclass\n",
        "import math\n",
        "from torch.distributed import init_process_group\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "eKHrOp0seleB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1WzzXEvY3TFj"
      },
      "outputs": [],
      "source": [
        "with  open('/content/input (2).txt', mode = 'r', encoding = 'utf-8') as f:\n",
        "  text =  f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKWws-Jm3eGk",
        "outputId": "ef39ed7e-d88f-4e50-bd36-93dc698b66d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XLu51zn03sxh"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text[:])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkA8PF-N4LH6",
        "outputId": "89e26131-fe56-48d4-f327-66c973b5eace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique chars:65\n",
            "----------------------\n",
            "lenght of text:1115394\n"
          ]
        }
      ],
      "source": [
        "print(f'unique chars:{len(chars)}')\n",
        "print(f'----------------------')\n",
        "print(f'lenght of text:{len(text)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpO52ewP5wg2",
        "outputId": "b664eac8-1cb5-4107-baf4-29ddf318e11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XsRYA5508iEW"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_ad5FnOZ8b04"
      },
      "outputs": [],
      "source": [
        "enc =  tiktoken.encoding_for_model(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "gyYQJqCU8XHi"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "  return enc.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UQx6CC208whf"
      },
      "outputs": [],
      "source": [
        "def decode(ids):\n",
        "  return enc.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TIdg7yVF9Jyj"
      },
      "outputs": [],
      "source": [
        "stoi = {i:v for v, i in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_wJpfnKn9HV0"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "\n",
        "  def __init__(self , B, T):\n",
        "    self.B    =  B\n",
        "    self.T    =  T\n",
        "    self.path = \"/content/input (2).txt\"\n",
        "    self.tokenizer =  tiktoken.encoding_for_model(\"gpt2\")\n",
        "\n",
        "    with open(self.path , mode =  'r', encoding = 'utf-8') as f:\n",
        "      text  =  f.read()\n",
        "    self.tokens =  self.tokenizer.encode(text[:])\n",
        "    self.tokens =  torch.tensor(self.tokens,dtype  = torch.long)\n",
        "    print(f'Loading :{len(self.tokens)} tokens')\n",
        "    print(f'1 Epoch = {len(self.tokens) // self.B * self.T}')\n",
        "    self.current_batch =  0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf  = self.tokens[self.current_batch:B*T+1+self.current_batch]\n",
        "    x    = buf[:-1].view(B, T)\n",
        "    y    = buf[1:].view(B, T)\n",
        "    self.current_batch+=B*T\n",
        "    if len(self.tokens) < (B*T+1) + self.current_batch:\n",
        "      self.current_batch =  0\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARgE9S-pJUc-",
        "outputId": "96907715-6b44-4517-cec5-d416abc9106c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device =  \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "print(f\"Using device:{device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "L-I8oDfiLfyB"
      },
      "outputs": [],
      "source": [
        "enc =  tiktoken.encoding_for_model('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dZroN3YZC7Av"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  vocab_size:int = enc.n_vocab\n",
        "  n_embd:int = 64\n",
        "  head_size:int = 64\n",
        "  n_layers:int= 8\n",
        "  n_heads:int=8\n",
        "  block_size:int = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cMHCVIVaEvuU"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self,head_size):\n",
        "\n",
        "    super().__init__()\n",
        "    self.config =  GPTConfig()\n",
        "    self.head_size =  head_size\n",
        "    self.query     =  nn.Linear(self.config.n_embd, self.head_size, bias  =  False)\n",
        "    self.key       =  nn.Linear(self.config.n_embd, self.head_size, bias  =  False)\n",
        "    self.value     =  nn.Linear(self.config.n_embd, self.head_size, bias  =  False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(self.config.block_size, self.config.block_size)))\n",
        "    self.wei =  0\n",
        "\n",
        "  def forward(self , x):\n",
        "\n",
        "    B,T,C =  x.shape\n",
        "    q = self.query(x) #INPUT-x-->(B,T,C) @ (C,H) ----> (B*T,C) @ (C,H)--->(B,T,H)\n",
        "    k = self.key(x)   #INPUT-x-->(B,T,C) @ (C,H) ----> (B*T,C) @ (C,H)--->(B,T,H)\n",
        "    v = self.value(x) #INPUT-x-->(B,T,C) @ (C,H) ----> (B*T,C) @ (C,H)--->(B,T,H)\n",
        "\n",
        "    self.wei =  q @ k.transpose(-2,-1)*C**-0.5 #(B,T,H ) @(B,T,H)--->(B,T,H) @ (B,H,T)---->(B,T,T)\n",
        "    wei =  self.wei.masked_fill(self.tril[:T,:T]==0 , float('-inf')) # (B,T ,T)\n",
        "    wei =  F.softmax(wei , dim=-1)\n",
        "    out =  wei @ v  #(B, T, T) @ (B, T, H)---->(B,T,H)\n",
        "    #out =  F.scaled_dot_product_attention(q, k, v, is_causal =  True) #(B, T, H) #  FLASH_ATTENTION\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2iSQ4UebUlqV"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.config    = GPTConfig()\n",
        "    self.head_size = self.config.n_embd // self.config.n_heads\n",
        "    self.heads     = nn.ModuleList([Attention(self.head_size) for _ in range(self.config.n_heads)])\n",
        "    self.proj      = nn.Linear(self.head_size * self.config.n_heads ,self.config.n_embd)\n",
        "    self.proj.NANOGPT_SCALE_INIT =  1\n",
        "\n",
        "  def  forward(self, x):\n",
        "    out =  torch.cat([h(x) for h in self.heads], dim =  -1)\n",
        "    return self.proj(out) # (B*T,H) @ (C,C)---> (B,T,H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lEIcEVdMZK6p"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.config = GPTConfig()\n",
        "    self.mlp   = nn.Linear(self.config.n_embd,4 * self.config.n_embd)\n",
        "    self.gelu  = nn.GELU()\n",
        "    self.proj  = nn.Linear(4 *  self.config.n_embd, self.config.n_embd)\n",
        "    self.proj.NANOGPT_SCALE_INIT=1\n",
        "\n",
        "  def forward(self  ,x):\n",
        "    x = self.mlp(x)  # (B,T,C) @ (C,C*4)---->(B,T,C*4)\n",
        "    x = self.gelu(x) # (B,T,C) ----> (B,T,C)\n",
        "    x = self.proj(x) # (B,T,C*4) @ (C*4,C) ----- >(B,T,C)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "oJ9p6c9Jcws9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.config  = GPTConfig()\n",
        "    self.mlp =  MLP()\n",
        "    self.mha = MultiHeadAttention()\n",
        "    self.ln1 = nn.LayerNorm(self.config.n_embd)\n",
        "    self.ln2 = nn.LayerNorm(self.config.n_embd)\n",
        "\n",
        "  def forward(self , x):\n",
        "    x =  x +  self.mha(self.ln1(x)) #INPUT---> (B,T,C/n_heads)--->OUTPUT->(B,T,C*n_heads) + (B,T,C)\n",
        "    x =  x +  self.mlp(self.ln2(x)) #INPUT(B,T,C)---->OUTPUT(B,T,C)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8X68_SkUBMYF"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config =  config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        bl  = nn.ModuleList([Block() for _ in range(self.config.n_layers)]),\n",
        "        lnu = nn.LayerNorm(self.config.n_embd)\n",
        "    ))\n",
        "\n",
        "    self.lin  = nn.Linear(self.config.n_embd, self.config.vocab_size)\n",
        "    self.transformer.wte.weight =  self.lin.weight\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "  def forward(self , idx, targets =  None):\n",
        "    B, T       =  idx.shape\n",
        "    tokens_emb =  self.transformer.wte(idx) # INPUT (B,T)----> OUTPUT (B,T,C)\n",
        "    pos_emb    =  self.transformer.wpe(torch.arange(T, device = device)) #INPUT(T)--->OUTPUT(B,T)\n",
        "    x          =  tokens_emb +  pos_emb #(B, T) + (B, T, C)----> (B,T ,C)\n",
        "    for h in self.transformer.bl:\n",
        "      x = h(x)\n",
        "    logits     =  self.lin(x) #INPUT(B, T, C)--->(B*T,C)----->(B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B, T ,C =  logits.shape\n",
        "      logits  = logits.view(B*T, C) #   (B,T ,C)---->(B*T,C)\n",
        "      targets = targets.view(B*T)   #   (B, T)--->(B)\n",
        "      loss    =  F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module , nn.Linear):\n",
        "      std=0.02\n",
        "      if hasattr(module ,\"NANOGPT_SCALE_INIT\"):\n",
        "        std*=(2*self.config.n_layers)**-0.5\n",
        "      torch.nn.init.normal_(module.weight, mean =  0.0 , std = std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module , nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean = 0.0 ,std =  0.02)\n",
        "\n",
        "  def configure_optimizers(self, weight_decay , device, learning_rate):\n",
        "    param_dict = {pn:p for pn,p  in self.named_parameters()}\n",
        "    param_dict = {pn:p for pn,p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "    decayed_params   = [p for n,p in param_dict.items() if p.ndim >=2 ]\n",
        "    nodecayed_params = [p for n,p in param_dict.items() if p.ndim <=1 ]\n",
        "\n",
        "    options_groups = [\n",
        "        {\"params\":decayed_params,\"weight_decay\":weight_decay},\n",
        "        {\"params\":nodecayed_params,\"weight_decay\":0.0}\n",
        "    ]\n",
        "\n",
        "    num_decayed_params   = sum(p.numel() for p  in decayed_params)\n",
        "    num_nodecayed_params = sum(p.numel() for p  in nodecayed_params)\n",
        "\n",
        "    print(f'num tensor decayed_params:{len(decayed_params)} with:{num_decayed_params}')\n",
        "    print(f'------------------------------------------------')\n",
        "    print(f'num no tensor decayed_params:{len(nodecayed_params)} with:{num_nodecayed_params}')\n",
        "\n",
        "    fused_available =  \"fused\"  in inspect.signature(torch.optim.AdamW).parameters\n",
        "    using_fused           = fused_available and \"cuda\" in device\n",
        "    print(f\"Using fused AdamW:{using_fused}\")\n",
        "\n",
        "    optim = torch.optim.AdamW(options_groups,lr = learning_rate, betas = (0.9,0.95),eps = 1e-8,fused = using_fused)\n",
        "    return optim\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      idx_cond      = idx[:,-GPTConfig.block_size:]\n",
        "      logits,  loss =  self(idx_cond)\n",
        "      logits        =  logits[:,-1,:] #(B,T,C) ---->(B,C)\n",
        "      probs         =  F.softmax(logits, dim =-1 ) #(B, C)------>(B,C)\n",
        "      next_idx      =  torch.multinomial(probs, num_samples =  1) #(B,C)--->(B,1)\n",
        "      idx           =  torch.cat([idx,next_idx], dim=1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "9VeYoLBVwHlh"
      },
      "outputs": [],
      "source": [
        "gpt2 =  GPT2(GPTConfig)\n",
        "gpt2.to(device)\n",
        "gpt2 = torch.compile(gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "B1K_oIQgvUqm"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CosineDecayParamaters:\n",
        "  max_steps:int=1000\n",
        "  max_lr:float=3e-4\n",
        "  min_lr:float=max_lr * 0.1\n",
        "  warmup_steps:int=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMP40_cW0X2v",
        "outputId": "34f1ad6f-e8f2-4e00-b647-05b106a469a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num tensor decayed_params:218 with:3610176\n",
            "------------------------------------------------\n",
            "num no tensor decayed_params:59 with:55505\n",
            "Using fused AdamW:False\n"
          ]
        }
      ],
      "source": [
        "op    =  CosineDecayParamaters()\n",
        "optim = gpt2.configure_optimizers(0.01,device, learning_rate = 6e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3t5x7RG253f",
        "outputId": "498f3985-9068-408b-fb3c-1a99673110e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading :338025 tokens\n",
            "1 Epoch = 676048\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "D = DataLoaderLite(4,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "xTGOaIK54h8o"
      },
      "outputs": [],
      "source": [
        "ddp = int(os.environ.get('RANK',-1))!=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSVMUtA54dus",
        "outputId": "a7f2905d-55fe-4c57-bb2d-e156cfdbe5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device:cpu\n"
          ]
        }
      ],
      "source": [
        "if ddp :\n",
        "  assert torch.cuda.is_available()\n",
        "  init_process_group(backend='nccl')\n",
        "  ddp_rank = int(os.environ['RANK'])\n",
        "  ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "  ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "  device         = f\"cuda:{ddp_local_rank}\"\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank==0\n",
        "else:\n",
        "  ddp_rank =  0\n",
        "  ddp_local_rank = 0\n",
        "  master_process =  True\n",
        "  ddp_world_size = 1\n",
        "\n",
        "  device =  \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif  hasattr(torch.backends,\"mps\") and torch.backends.mps.is_available():\n",
        "    device =\"mps\"\n",
        "  print(f'using device:{device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czbv9V8x8lnJ",
        "outputId": "c6241459-dc5f-448b-c2c1-c4f26056dab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total batches:512\n",
            "gradient accumulation steps:16\n"
          ]
        }
      ],
      "source": [
        "TOTAL_BATCHES = 512\n",
        "assert TOTAL_BATCHES % (D.B*D.T*ddp_world_size) == 0\n",
        "grad_acc = TOTAL_BATCHES //  (D.B * D.T * ddp_world_size)\n",
        "print(f'total batches:{TOTAL_BATCHES}')\n",
        "print(f'gradient accumulation steps:{grad_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "yUQ9ZkfvwWkQ"
      },
      "outputs": [],
      "source": [
        "def get_lr(it):\n",
        "  if op.warmup_steps > it:\n",
        "    return op.max_lr * (it + 1) / op.warmup_steps\n",
        "\n",
        "  if it >  op.max_steps:\n",
        "    return op.min_lr\n",
        "\n",
        "  decay_ratio = (it - op.warmup_steps) / (op.max_steps -  op.warmup_steps)\n",
        "  assert 0<=decay_ratio<=1\n",
        "  coeff =  0.5*(1.0 +  math.cos(math.pi * decay_ratio))\n",
        "  return op.min_lr + coeff*(op.max_lr - op.min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kbPXjFAsBRg",
        "outputId": "f4c00de8-8cf6-49cc-d3cb-dd7414dc3847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 10.8239 | norm:0.2267 | lr 0.0000\n",
            "loss 10.8239 | norm:0.2025 | lr 0.0001\n",
            "loss 10.8221 | norm:0.2272 | lr 0.0001\n",
            "loss 10.8204 | norm:0.2170 | lr 0.0001\n",
            "loss 10.8184 | norm:0.2259 | lr 0.0001\n",
            "loss 10.8135 | norm:0.2599 | lr 0.0002\n",
            "loss 10.8049 | norm:0.3125 | lr 0.0002\n",
            "loss 10.7993 | norm:0.2950 | lr 0.0002\n",
            "loss 10.7907 | norm:0.3120 | lr 0.0003\n",
            "loss 10.7734 | norm:0.3912 | lr 0.0003\n",
            "loss 10.7501 | norm:0.4920 | lr 0.0003\n",
            "loss 10.7175 | norm:0.6236 | lr 0.0003\n",
            "loss 10.7023 | norm:0.6095 | lr 0.0003\n",
            "loss 10.6462 | norm:0.8406 | lr 0.0003\n",
            "loss 10.6121 | norm:0.8943 | lr 0.0003\n",
            "loss 10.5466 | norm:1.0890 | lr 0.0003\n",
            "loss 10.5114 | norm:1.1314 | lr 0.0003\n",
            "loss 10.4447 | norm:1.2600 | lr 0.0003\n",
            "loss 10.3609 | norm:1.4479 | lr 0.0003\n",
            "loss 10.2377 | norm:1.8688 | lr 0.0003\n",
            "loss 10.1311 | norm:1.9408 | lr 0.0003\n",
            "loss 9.9658 | norm:2.4852 | lr 0.0003\n",
            "loss 9.9991 | norm:2.0644 | lr 0.0003\n",
            "loss 9.7494 | norm:2.5889 | lr 0.0003\n",
            "loss 9.5276 | norm:3.0951 | lr 0.0003\n",
            "loss 9.3523 | norm:3.4839 | lr 0.0003\n",
            "loss 9.2714 | norm:3.7416 | lr 0.0003\n",
            "loss 8.9376 | norm:3.9325 | lr 0.0003\n",
            "loss 8.8033 | norm:4.0500 | lr 0.0003\n",
            "loss 8.3617 | norm:4.8848 | lr 0.0003\n",
            "loss 8.4465 | norm:4.1211 | lr 0.0003\n",
            "loss 7.8186 | norm:5.3243 | lr 0.0003\n",
            "loss 7.6538 | norm:4.7830 | lr 0.0003\n",
            "loss 7.3186 | norm:4.2379 | lr 0.0003\n",
            "loss 6.9946 | norm:4.3464 | lr 0.0003\n",
            "loss 7.3083 | norm:6.2570 | lr 0.0003\n",
            "loss 6.6517 | norm:10.2229 | lr 0.0003\n",
            "loss 7.2281 | norm:18.1992 | lr 0.0003\n",
            "loss 7.4935 | norm:23.5039 | lr 0.0003\n",
            "loss 7.0568 | norm:27.0261 | lr 0.0003\n",
            "loss 6.8870 | norm:29.4694 | lr 0.0003\n",
            "loss 7.3319 | norm:33.6271 | lr 0.0003\n",
            "loss 7.7409 | norm:35.3798 | lr 0.0003\n",
            "loss 7.9965 | norm:36.5456 | lr 0.0003\n",
            "loss 7.4350 | norm:34.0313 | lr 0.0003\n",
            "loss 6.9541 | norm:32.0863 | lr 0.0003\n",
            "loss 7.0379 | norm:30.4412 | lr 0.0003\n",
            "loss 6.8811 | norm:27.6740 | lr 0.0003\n",
            "loss 7.3608 | norm:26.7810 | lr 0.0003\n",
            "loss 7.0816 | norm:22.5936 | lr 0.0003\n",
            "loss 6.6519 | norm:18.5748 | lr 0.0003\n",
            "loss 7.4625 | norm:17.9155 | lr 0.0003\n",
            "loss 7.2161 | norm:14.4706 | lr 0.0003\n",
            "loss 6.8155 | norm:10.8370 | lr 0.0003\n",
            "loss 6.6363 | norm:8.4922 | lr 0.0003\n",
            "loss 6.7520 | norm:6.6476 | lr 0.0003\n",
            "loss 6.3227 | norm:5.3819 | lr 0.0003\n"
          ]
        }
      ],
      "source": [
        "for steps in range(op.max_steps):\n",
        "  loss_acc = 0.0\n",
        "  optim.zero_grad(set_to_none = True)\n",
        "  for grad_steps in range(grad_acc):\n",
        "    xb,   yb = D.next_batch()\n",
        "    xb,  yb  =  xb.to(device), yb.to(device)\n",
        "    with torch.autocast(device_type =  device, dtype = torch.bfloat16):\n",
        "        logits , loss = gpt2(xb, yb)\n",
        "        loss =  loss / grad_acc\n",
        "        loss_acc+= loss.detach()\n",
        "    loss.backward()\n",
        "  norm =  torch.nn.utils.clip_grad_norm_(gpt2.parameters(), 0.1)\n",
        "  lr   = get_lr(steps)\n",
        "  for p in optim.param_groups:\n",
        "    p['lr'] = lr\n",
        "  optim.step()\n",
        "  print(f'loss {loss_acc.item():.4f} | norm:{norm:.4f} | lr {lr:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK7ttTe0naC1"
      },
      "outputs": [],
      "source": [
        "print(decode(gpt2.generate(torch.zeros(1,device=device ,dtype =  torch.long).repeat(2).unsqueeze(1),max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict    =   {pn:p for pn,p in gpt2.named_parameters()}\n",
        "param_dict    =   {pn:p for pn,p in param_dict.items() if p.requires_grad}"
      ],
      "metadata": {
        "id": "_fOA6rZgZewQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att = gpt2.transformer.bl[0].mha.heads[0].wei[0]"
      ],
      "metadata": {
        "id": "VUQ1l5ARZz4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb , yb = D.next_batch()"
      ],
      "metadata": {
        "id": "T7RsikRWc7tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb[0]"
      ],
      "metadata": {
        "id": "o7EHqHUCiKGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens =  [i.item() for i in xb[0]]"
      ],
      "metadata": {
        "id": "Ae5rZd6fhjMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "K1G_alg7eGPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = []\n",
        "for t in tokens:\n",
        "  x = decode([t])\n",
        "  T.append(x)"
      ],
      "metadata": {
        "id": "CX73pHd9hobb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T"
      ],
      "metadata": {
        "id": "B8DHb4jUjsU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_transpose =  att.T"
      ],
      "metadata": {
        "id": "3Y8uJQmUi8Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att"
      ],
      "metadata": {
        "id": "7W3BmeIgnSu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_transpose = att"
      ],
      "metadata": {
        "id": "1E-kqP2FjHR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_matrix =  pd.DataFrame({\n",
        "    f\"{T[0]}\":[i.item() for i  in att_transpose[:,0]],\n",
        "    f\"{T[1]}\":[i.item() for i  in att_transpose[:,1]],\n",
        "    f\"{T[2]}\":[i.item() for i  in att_transpose[:,2]],\n",
        "    f\"{T[3]}\":[i.item() for i  in att_transpose[:,3]],\n",
        "    f\"{T[4]}\":[i.item() for i  in att_transpose[:,4]],\n",
        "    f\"{T[5]}\":[i.item() for i  in att_transpose[:,5]],\n",
        "    f\"{T[6]}\":[i.item() for i  in att_transpose[:,6]],\n",
        "    f\"{T[7]}\":[i.item() for i  in att_transpose[:,7]],\n",
        "})"
      ],
      "metadata": {
        "id": "IncB4wp8kbo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_matrix"
      ],
      "metadata": {
        "id": "bEtXNRIalN2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_matrix_transpose = {}\n",
        "for p in range(8):\n",
        "  att_matrix_transpose[T[p]] = [i.item() for i in att_matrix[T,:]]"
      ],
      "metadata": {
        "id": "zQfANVOD1WUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_matrix"
      ],
      "metadata": {
        "id": "mrmVk6CT1_qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LpdxjNj9lVX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A =  np.asarray(att_matrix.iloc[0,:])"
      ],
      "metadata": {
        "id": "nWtgc_dflpkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A"
      ],
      "metadata": {
        "id": "OBjCkt9Rl3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(A,axis =  0)"
      ],
      "metadata": {
        "id": "3FbVvPfklXEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix =  att_matrix.corr()"
      ],
      "metadata": {
        "id": "M4GeZr8vmJGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix"
      ],
      "metadata": {
        "id": "-kCXXx4cmz-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(att_matrix,xticklabels=corr_matrix.columns.values , yticklabels=corr_matrix.columns.values,annot=True)"
      ],
      "metadata": {
        "id": "uvqSuOG5mPGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}