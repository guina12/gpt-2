{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxrJolld2pkE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import seaborn\n",
        "import inspect\n",
        "import torch.nn as nn\n",
        "from dataclasses import  dataclass\n",
        "import math\n",
        "from torch.distributed import init_process_group\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKHrOp0seleB"
      },
      "outputs": [],
      "source": [
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WzzXEvY3TFj"
      },
      "outputs": [],
      "source": [
        "with  open('/content/input (2).txt', mode = 'r', encoding = 'utf-8') as f:\n",
        "  text =  f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKWws-Jm3eGk",
        "outputId": "f07c5764-2e3c-4327-b9ac-36def7f549a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLu51zn03sxh"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text[:])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkA8PF-N4LH6",
        "outputId": "b85f9c5b-07b5-46ac-bc2b-997f00619375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique chars:65\n",
            "----------------------\n",
            "lenght of text:1115394\n"
          ]
        }
      ],
      "source": [
        "print(f'unique chars:{len(chars)}')\n",
        "print(f'----------------------')\n",
        "print(f'lenght of text:{len(text)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpO52ewP5wg2",
        "outputId": "e441f954-68b5-419b-b458-b85c2eb15b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsRYA5508iEW"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ad5FnOZ8b04"
      },
      "outputs": [],
      "source": [
        "enc =  tiktoken.encoding_for_model(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyYQJqCU8XHi"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "  return enc.encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQx6CC208whf"
      },
      "outputs": [],
      "source": [
        "def decode(ids):\n",
        "  return enc.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIdg7yVF9Jyj"
      },
      "outputs": [],
      "source": [
        "stoi = {i:v for v, i in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wJpfnKn9HV0"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "\n",
        "  def __init__(self , B, T):\n",
        "    self.B    =  B\n",
        "    self.T    =  T\n",
        "    self.path = \"/content/input (2).txt\"\n",
        "    self.tokenizer =  tiktoken.encoding_for_model(\"gpt2\")\n",
        "\n",
        "    with open(self.path , mode =  'r', encoding = 'utf-8') as f:\n",
        "      text  =  f.read()\n",
        "    self.tokens =  self.tokenizer.encode(text[:])\n",
        "    self.tokens =  torch.tensor(self.tokens,dtype  = torch.long)\n",
        "    print(f'Loading :{len(self.tokens)} tokens')\n",
        "    print(f'1 Epoch = {len(self.tokens) // self.B * self.T}')\n",
        "    self.current_batch =  0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf  = self.tokens[self.current_batch:B*T+1+self.current_batch]\n",
        "    x    = buf[:-1].view(B, T)\n",
        "    y    = buf[1:].view(B, T)\n",
        "    self.current_batch+=B*T\n",
        "    if len(self.tokens) < (B*T+1) + self.current_batch:\n",
        "      self.current_batch =  0\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D = DataLoaderLite(4,8)\n",
        "xb, yb = D.next_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA16ki1b5_ib",
        "outputId": "504cd396-4928-4647-c8ab-c11258523e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading :338025 tokens\n",
            "1 Epoch = 676048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb"
      ],
      "metadata": {
        "id": "Mu6GaTRC6I0b",
        "outputId": "14a7978c-b026-4e79-f390-1eeb6e92733f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5962, 22307,    25,   198,  8421,   356,  5120,   597],\n",
              "        [ 2252,    11,  3285,   502,  2740,    13,   198,   198],\n",
              "        [ 3237,    25,   198,  5248,   461,    11,  2740,    13],\n",
              "        [  198,   198,  5962, 22307,    25,   198,  1639,   389]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARgE9S-pJUc-",
        "outputId": "adc57190-c8bc-4dd7-89a2-3702d77c93d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device =  \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "print(f\"Using device:{device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-I8oDfiLfyB"
      },
      "outputs": [],
      "source": [
        "enc =  tiktoken.encoding_for_model('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZroN3YZC7Av"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "  vocab_size:int = enc.n_vocab\n",
        "  n_embd:int = 64\n",
        "  head_size:int = 64\n",
        "  n_layers:int= 8\n",
        "  n_heads:int=8\n",
        "  block_size:int = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMHCVIVaEvuU"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self,head_size):\n",
        "\n",
        "    super().__init__()\n",
        "    self.config =  GPTConfig()\n",
        "    self.head_size =  head_size\n",
        "    self.query     =  nn.Linear(self.config.n_embd, self.head_size, bias  =  False)\n",
        "    self.key       =  nn.Linear(self.config.n_embd, self.head_size, bias  =  False)\n",
        "    self.value     =  nn.Linear(self.config.n_embd, self.head_size, bias  =  False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(self.config.block_size, self.config.block_size)))\n",
        "    self.wei =  0\n",
        "\n",
        "  def forward(self , x):\n",
        "\n",
        "    B,T,C =  x.shape\n",
        "    q = self.query(x) #INPUT-x-->(B,T,C) @ (C,H) ----> (B*T,C) @ (C,H)--->(B,T,H)\n",
        "    k = self.key(x)   #INPUT-x-->(B,T,C) @ (C,H) ----> (B*T,C) @ (C,H)--->(B,T,H)\n",
        "    v = self.value(x) #INPUT-x-->(B,T,C) @ (C,H) ----> (B*T,C) @ (C,H)--->(B,T,H)\n",
        "\n",
        "    self.wei =  q @ k.transpose(-2,-1)*C**-0.5 #(B,T,H ) @(B,T,H)--->(B,T,H) @ (B,H,T)---->(B,T,T)\n",
        "    wei =  self.wei.masked_fill(self.tril[:T,:T]==0 , float('-inf')) # (B,T ,T)\n",
        "    wei =  F.softmax(wei , dim=-1)\n",
        "    out =  wei @ v  #(B, T, T) @ (B, T, H)---->(B,T,H)\n",
        "    #out =  F.scaled_dot_product_attention(q, k, v, is_causal =  True) #(B, T, H) #  FLASH_ATTENTION\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iSQ4UebUlqV"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.config    = GPTConfig()\n",
        "    self.head_size = self.config.n_embd // self.config.n_heads\n",
        "    self.heads     = nn.ModuleList([Attention(self.head_size) for _ in range(self.config.n_heads)])\n",
        "    self.proj      = nn.Linear(self.head_size * self.config.n_heads ,self.config.n_embd)\n",
        "    self.proj.NANOGPT_SCALE_INIT =  1\n",
        "\n",
        "  def  forward(self, x):\n",
        "    out =  torch.cat([h(x) for h in self.heads], dim =  -1)\n",
        "    return self.proj(out) # (B*T,H) @ (C,C)---> (B,T,H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEIcEVdMZK6p"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "    self.config = GPTConfig()\n",
        "    self.mlp   = nn.Linear(self.config.n_embd,4 * self.config.n_embd)\n",
        "    self.gelu  = nn.GELU()\n",
        "    self.proj  = nn.Linear(4 *  self.config.n_embd, self.config.n_embd)\n",
        "    self.proj.NANOGPT_SCALE_INIT=1\n",
        "\n",
        "  def forward(self  ,x):\n",
        "    x = self.mlp(x)  # (B,T,C) @ (C,C*4)---->(B,T,C*4)\n",
        "    x = self.gelu(x) # (B,T,C) ----> (B,T,C)\n",
        "    x = self.proj(x) # (B,T,C*4) @ (C*4,C) ----- >(B,T,C)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ9p6c9Jcws9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.config  = GPTConfig()\n",
        "    self.mlp =  MLP()\n",
        "    self.mha = MultiHeadAttention()\n",
        "    self.ln1 = nn.LayerNorm(self.config.n_embd)\n",
        "    self.ln2 = nn.LayerNorm(self.config.n_embd)\n",
        "\n",
        "  def forward(self , x):\n",
        "    x =  x +  self.mha(self.ln1(x)) #INPUT---> (B,T,C/n_heads)--->OUTPUT->(B,T,C*n_heads) + (B,T,C)\n",
        "    x =  x +  self.mlp(self.ln2(x)) #INPUT(B,T,C)---->OUTPUT(B,T,C)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X68_SkUBMYF"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config =  config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        bl  = nn.ModuleList([Block() for _ in range(self.config.n_layers)]),\n",
        "        lnu = nn.LayerNorm(self.config.n_embd)\n",
        "    ))\n",
        "\n",
        "    self.lin  = nn.Linear(self.config.n_embd, self.config.vocab_size)\n",
        "    self.transformer.wte.weight =  self.lin.weight\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "  def forward(self , idx, targets =  None):\n",
        "    B, T       =  idx.shape\n",
        "    tokens_emb =  self.transformer.wte(idx) # INPUT (B,T)----> OUTPUT (B,T,C)\n",
        "    pos_emb    =  self.transformer.wpe(torch.arange(T, device = device)) #INPUT(T)--->OUTPUT(B,T)\n",
        "    x          =  tokens_emb +  pos_emb #(B, T) + (B, T, C)----> (B,T ,C)\n",
        "    for h in self.transformer.bl:\n",
        "      x = h(x)\n",
        "    logits     =  self.lin(x) #INPUT(B, T, C)--->(B*T,C)----->(B,T,C)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "\n",
        "    else:\n",
        "      B, T ,C =  logits.shape\n",
        "      logits  = logits.view(B*T, C) #   (B,T ,C)---->(B*T,C)\n",
        "      targets = targets.view(B*T)   #   (B, T)--->(B)\n",
        "      loss    =  F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module , nn.Linear):\n",
        "      std=0.02\n",
        "      if hasattr(module ,\"NANOGPT_SCALE_INIT\"):\n",
        "        std*=(2*self.config.n_layers)**-0.5\n",
        "      torch.nn.init.normal_(module.weight, mean =  0.0 , std = std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module , nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean = 0.0 ,std =  0.02)\n",
        "\n",
        "  def configure_optimizers(self, weight_decay , device, learning_rate):\n",
        "    param_dict = {pn:p for pn,p  in self.named_parameters()}\n",
        "    param_dict = {pn:p for pn,p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "    decayed_params   = [p for n,p in param_dict.items() if p.ndim >=2 ]\n",
        "    nodecayed_params = [p for n,p in param_dict.items() if p.ndim <=1 ]\n",
        "\n",
        "    options_groups = [\n",
        "        {\"params\":decayed_params,\"weight_decay\":weight_decay},\n",
        "        {\"params\":nodecayed_params,\"weight_decay\":0.0}\n",
        "    ]\n",
        "\n",
        "    num_decayed_params   = sum(p.numel() for p  in decayed_params)\n",
        "    num_nodecayed_params = sum(p.numel() for p  in nodecayed_params)\n",
        "\n",
        "    print(f'num tensor decayed_params:{len(decayed_params)} with:{num_decayed_params}')\n",
        "    print(f'------------------------------------------------')\n",
        "    print(f'num no tensor decayed_params:{len(nodecayed_params)} with:{num_nodecayed_params}')\n",
        "\n",
        "    fused_available =  \"fused\"  in inspect.signature(torch.optim.AdamW).parameters\n",
        "    using_fused           = fused_available and \"cuda\" in device\n",
        "    print(f\"Using fused AdamW:{using_fused}\")\n",
        "\n",
        "    optim = torch.optim.AdamW(options_groups,lr = learning_rate, betas = (0.9,0.95),eps = 1e-8,fused = using_fused)\n",
        "    return optim\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for i in range(max_new_tokens):\n",
        "      idx_cond      = idx[:,-GPTConfig.block_size:]\n",
        "      logits,  loss =  self(idx_cond)\n",
        "      logits        =  logits[:,-1,:] #(B,T,C) ---->(B,C)\n",
        "      probs         =  F.softmax(logits, dim =-1 ) #(B, C)------>(B,C)\n",
        "      next_idx      =  torch.multinomial(probs, num_samples =  1) #(B,C)--->(B,1)\n",
        "      idx           =  torch.cat([idx,next_idx], dim=1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VeYoLBVwHlh"
      },
      "outputs": [],
      "source": [
        "gpt2 =  GPT2(GPTConfig)\n",
        "gpt2.to(device)\n",
        "gpt2 = torch.compile(gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1K_oIQgvUqm"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CosineDecayParamaters:\n",
        "  max_steps:int=1000\n",
        "  max_lr:float=3e-4\n",
        "  min_lr:float=max_lr * 0.1\n",
        "  warmup_steps:int=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMP40_cW0X2v",
        "outputId": "26b92cb4-bb79-42c7-f26f-b9e706e43b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num tensor decayed_params:218 with:3610176\n",
            "------------------------------------------------\n",
            "num no tensor decayed_params:59 with:55505\n",
            "Using fused AdamW:True\n"
          ]
        }
      ],
      "source": [
        "op    =  CosineDecayParamaters()\n",
        "optim = gpt2.configure_optimizers(0.01,device, learning_rate = 6e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3t5x7RG253f",
        "outputId": "561ac2e5-c160-40d6-c537-79598e90963e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading :338025 tokens\n",
            "1 Epoch = 676048\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "D = DataLoaderLite(4,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTGOaIK54h8o"
      },
      "outputs": [],
      "source": [
        "ddp = int(os.environ.get('RANK',-1))!=-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSVMUtA54dus",
        "outputId": "d983edb1-4243-4463-feb4-df5c9c02d7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device:cuda\n"
          ]
        }
      ],
      "source": [
        "if ddp :\n",
        "  assert torch.cuda.is_available()\n",
        "  init_process_group(backend='nccl')\n",
        "  ddp_rank = int(os.environ['RANK'])\n",
        "  ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "  ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "  device         = f\"cuda:{ddp_local_rank}\"\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank==0\n",
        "else:\n",
        "  ddp_rank =  0\n",
        "  ddp_local_rank = 0\n",
        "  master_process =  True\n",
        "  ddp_world_size = 1\n",
        "\n",
        "  device =  \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif  hasattr(torch.backends,\"mps\") and torch.backends.mps.is_available():\n",
        "    device =\"mps\"\n",
        "  print(f'using device:{device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czbv9V8x8lnJ",
        "outputId": "331f51f6-5149-4bdb-939c-fabc812698fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total batches:512\n",
            "gradient accumulation steps:16\n"
          ]
        }
      ],
      "source": [
        "TOTAL_BATCHES = 512\n",
        "assert TOTAL_BATCHES % (D.B*D.T*ddp_world_size) == 0\n",
        "grad_acc = TOTAL_BATCHES //  (D.B * D.T * ddp_world_size)\n",
        "print(f'total batches:{TOTAL_BATCHES}')\n",
        "print(f'gradient accumulation steps:{grad_acc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUQ9ZkfvwWkQ"
      },
      "outputs": [],
      "source": [
        "def get_lr(it):\n",
        "  if op.warmup_steps > it:\n",
        "    return op.max_lr * (it + 1) / op.warmup_steps\n",
        "\n",
        "  if it >  op.max_steps:\n",
        "    return op.min_lr\n",
        "\n",
        "  decay_ratio = (it - op.warmup_steps) / (op.max_steps -  op.warmup_steps)\n",
        "  assert 0<=decay_ratio<=1\n",
        "  coeff =  0.5*(1.0 +  math.cos(math.pi * decay_ratio))\n",
        "  return op.min_lr + coeff*(op.max_lr - op.min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5kbPXjFAsBRg",
        "outputId": "36058b22-899d-4751-dfcf-6b3808924b55"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0707 22:50:57.537000 260 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_fx.py:1948: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 10.8125 | norm:0.2154 | lr 0.0000\n",
            "loss 10.8125 | norm:0.1985 | lr 0.0001\n",
            "loss 10.8125 | norm:0.2228 | lr 0.0001\n",
            "loss 10.8125 | norm:0.2113 | lr 0.0001\n",
            "loss 10.8119 | norm:0.1947 | lr 0.0001\n",
            "loss 10.8070 | norm:0.2398 | lr 0.0002\n",
            "loss 10.8062 | norm:0.2847 | lr 0.0002\n",
            "loss 10.8010 | norm:0.2660 | lr 0.0002\n",
            "loss 10.7972 | norm:0.2853 | lr 0.0003\n",
            "loss 10.7794 | norm:0.3636 | lr 0.0003\n",
            "loss 10.7562 | norm:0.4611 | lr 0.0003\n",
            "loss 10.7360 | norm:0.5766 | lr 0.0003\n",
            "loss 10.7101 | norm:0.5706 | lr 0.0003\n",
            "loss 10.6694 | norm:0.7826 | lr 0.0003\n",
            "loss 10.6353 | norm:0.8395 | lr 0.0003\n",
            "loss 10.5745 | norm:1.0239 | lr 0.0003\n",
            "loss 10.5234 | norm:1.0841 | lr 0.0003\n",
            "loss 10.4756 | norm:1.1979 | lr 0.0003\n",
            "loss 10.3993 | norm:1.3776 | lr 0.0003\n",
            "loss 10.2789 | norm:1.7934 | lr 0.0003\n",
            "loss 10.1758 | norm:1.8680 | lr 0.0003\n",
            "loss 10.0295 | norm:2.3969 | lr 0.0003\n",
            "loss 10.0548 | norm:2.0018 | lr 0.0003\n",
            "loss 9.7983 | norm:2.5191 | lr 0.0003\n",
            "loss 9.6101 | norm:3.0122 | lr 0.0003\n",
            "loss 9.3602 | norm:3.4264 | lr 0.0003\n",
            "loss 9.3723 | norm:3.6623 | lr 0.0003\n",
            "loss 9.0355 | norm:3.8760 | lr 0.0003\n",
            "loss 8.7978 | norm:4.0187 | lr 0.0003\n",
            "loss 8.4729 | norm:4.8572 | lr 0.0003\n",
            "loss 8.4273 | norm:4.1491 | lr 0.0003\n",
            "loss 7.9038 | norm:5.4164 | lr 0.0003\n",
            "loss 7.6365 | norm:4.9824 | lr 0.0003\n",
            "loss 7.3146 | norm:4.6142 | lr 0.0003\n",
            "loss 6.9990 | norm:4.7838 | lr 0.0003\n",
            "loss 7.2972 | norm:4.2460 | lr 0.0003\n",
            "loss 6.6262 | norm:6.9075 | lr 0.0003\n",
            "loss 7.1397 | norm:13.7873 | lr 0.0003\n",
            "loss 7.5028 | norm:18.9416 | lr 0.0003\n",
            "loss 7.0798 | norm:22.4076 | lr 0.0003\n",
            "loss 6.8979 | norm:24.8157 | lr 0.0003\n",
            "loss 7.1461 | norm:29.0303 | lr 0.0003\n",
            "loss 7.7852 | norm:30.8456 | lr 0.0003\n",
            "loss 7.9179 | norm:31.9443 | lr 0.0003\n",
            "loss 7.6601 | norm:29.3856 | lr 0.0003\n",
            "loss 7.1064 | norm:27.3594 | lr 0.0003\n",
            "loss 7.1903 | norm:25.9559 | lr 0.0003\n",
            "loss 7.0841 | norm:23.6184 | lr 0.0003\n",
            "loss 7.5306 | norm:23.1941 | lr 0.0003\n",
            "loss 7.2050 | norm:19.8059 | lr 0.0003\n",
            "loss 6.7463 | norm:16.6216 | lr 0.0003\n",
            "loss 7.4539 | norm:16.6259 | lr 0.0003\n",
            "loss 7.2517 | norm:14.0195 | lr 0.0003\n",
            "loss 6.9133 | norm:11.0437 | lr 0.0003\n",
            "loss 6.7865 | norm:9.1145 | lr 0.0003\n",
            "loss 6.7923 | norm:7.1935 | lr 0.0003\n",
            "loss 6.4191 | norm:5.6072 | lr 0.0003\n",
            "loss 6.9852 | norm:5.0037 | lr 0.0003\n",
            "loss 7.0139 | norm:3.9998 | lr 0.0003\n",
            "loss 6.1519 | norm:5.4249 | lr 0.0003\n",
            "loss 6.0414 | norm:4.5249 | lr 0.0003\n",
            "loss 6.7361 | norm:4.4642 | lr 0.0003\n",
            "loss 6.5895 | norm:4.5708 | lr 0.0003\n",
            "loss 6.2312 | norm:6.7411 | lr 0.0003\n",
            "loss 6.4893 | norm:5.7698 | lr 0.0003\n",
            "loss 6.9830 | norm:3.6588 | lr 0.0003\n",
            "loss 6.9071 | norm:3.8095 | lr 0.0003\n",
            "loss 6.3724 | norm:5.9634 | lr 0.0003\n",
            "loss 6.8157 | norm:4.5071 | lr 0.0003\n",
            "loss 6.3809 | norm:4.8376 | lr 0.0003\n",
            "loss 6.3463 | norm:5.0242 | lr 0.0003\n",
            "loss 6.2293 | norm:4.8753 | lr 0.0003\n",
            "loss 6.2710 | norm:4.0643 | lr 0.0003\n",
            "loss 6.0322 | norm:3.4681 | lr 0.0003\n",
            "loss 6.5321 | norm:2.6451 | lr 0.0003\n",
            "loss 6.3753 | norm:2.5354 | lr 0.0003\n",
            "loss 6.2008 | norm:2.5380 | lr 0.0003\n",
            "loss 5.8536 | norm:2.5763 | lr 0.0003\n",
            "loss 6.5203 | norm:4.3778 | lr 0.0003\n",
            "loss 6.7102 | norm:4.9116 | lr 0.0003\n",
            "loss 6.2579 | norm:4.0445 | lr 0.0003\n",
            "loss 6.8555 | norm:4.7294 | lr 0.0003\n",
            "loss 6.5355 | norm:3.5465 | lr 0.0003\n",
            "loss 6.7495 | norm:3.7987 | lr 0.0003\n",
            "loss 6.7872 | norm:3.9967 | lr 0.0003\n",
            "loss 6.0470 | norm:2.2831 | lr 0.0003\n",
            "loss 6.5186 | norm:2.5763 | lr 0.0003\n",
            "loss 6.5415 | norm:2.7487 | lr 0.0003\n",
            "loss 6.5122 | norm:2.7925 | lr 0.0003\n",
            "loss 6.5137 | norm:3.0343 | lr 0.0003\n",
            "loss 6.4065 | norm:3.2033 | lr 0.0003\n",
            "loss 6.5506 | norm:3.4482 | lr 0.0003\n",
            "loss 7.4029 | norm:2.5368 | lr 0.0003\n",
            "loss 7.1317 | norm:2.7617 | lr 0.0003\n",
            "loss 7.2288 | norm:2.9069 | lr 0.0003\n",
            "loss 6.8869 | norm:2.9662 | lr 0.0003\n",
            "loss 7.2947 | norm:2.5008 | lr 0.0003\n",
            "loss 7.2126 | norm:2.7310 | lr 0.0003\n",
            "loss 7.2956 | norm:3.6234 | lr 0.0003\n",
            "loss 7.0360 | norm:3.6094 | lr 0.0003\n",
            "loss 7.1942 | norm:3.3871 | lr 0.0003\n",
            "loss 6.8035 | norm:3.1389 | lr 0.0003\n",
            "loss 6.9167 | norm:2.5071 | lr 0.0003\n",
            "loss 7.0518 | norm:2.0246 | lr 0.0003\n",
            "loss 7.0911 | norm:2.0379 | lr 0.0003\n",
            "loss 7.1146 | norm:1.7713 | lr 0.0003\n",
            "loss 6.9768 | norm:1.8955 | lr 0.0003\n",
            "loss 6.8220 | norm:1.9596 | lr 0.0003\n",
            "loss 6.9613 | norm:1.6777 | lr 0.0003\n",
            "loss 7.1069 | norm:2.2824 | lr 0.0003\n",
            "loss 6.9797 | norm:1.9272 | lr 0.0003\n",
            "loss 6.8298 | norm:1.7321 | lr 0.0003\n",
            "loss 6.6341 | norm:1.7372 | lr 0.0003\n",
            "loss 6.7660 | norm:1.5912 | lr 0.0003\n",
            "loss 7.1312 | norm:2.0799 | lr 0.0003\n",
            "loss 6.6927 | norm:1.6750 | lr 0.0003\n",
            "loss 6.7373 | norm:2.9377 | lr 0.0003\n",
            "loss 6.3535 | norm:2.6191 | lr 0.0003\n",
            "loss 6.7206 | norm:1.9112 | lr 0.0003\n",
            "loss 6.3160 | norm:1.9125 | lr 0.0003\n",
            "loss 6.4123 | norm:1.7554 | lr 0.0003\n",
            "loss 6.7486 | norm:1.8292 | lr 0.0003\n",
            "loss 6.6190 | norm:1.5943 | lr 0.0003\n",
            "loss 6.8144 | norm:1.7389 | lr 0.0003\n",
            "loss 6.4848 | norm:1.6329 | lr 0.0003\n",
            "loss 6.7862 | norm:1.5412 | lr 0.0003\n",
            "loss 6.9006 | norm:1.9361 | lr 0.0003\n",
            "loss 6.7003 | norm:1.7288 | lr 0.0003\n",
            "loss 6.6132 | norm:4.3571 | lr 0.0003\n",
            "loss 6.5761 | norm:10.0946 | lr 0.0003\n",
            "loss 7.0004 | norm:2.9399 | lr 0.0003\n",
            "loss 6.9070 | norm:3.8666 | lr 0.0003\n",
            "loss 6.6265 | norm:1.6314 | lr 0.0003\n",
            "loss 6.7490 | norm:1.4987 | lr 0.0003\n",
            "loss 6.6150 | norm:4.3181 | lr 0.0003\n",
            "loss 6.4485 | norm:2.9218 | lr 0.0003\n",
            "loss 6.5357 | norm:1.3673 | lr 0.0003\n",
            "loss 6.4698 | norm:1.8030 | lr 0.0003\n",
            "loss 6.4969 | norm:1.5624 | lr 0.0003\n",
            "loss 6.3389 | norm:1.4312 | lr 0.0003\n",
            "loss 6.2601 | norm:1.6828 | lr 0.0003\n",
            "loss 6.3901 | norm:1.4681 | lr 0.0003\n",
            "loss 6.3421 | norm:1.6005 | lr 0.0003\n",
            "loss 6.4991 | norm:1.6345 | lr 0.0003\n",
            "loss 6.6257 | norm:1.4746 | lr 0.0003\n",
            "loss 6.5018 | norm:1.5541 | lr 0.0003\n",
            "loss 6.5730 | norm:1.5781 | lr 0.0003\n",
            "loss 6.8431 | norm:1.4004 | lr 0.0003\n",
            "loss 6.6649 | norm:1.3734 | lr 0.0003\n",
            "loss 6.3466 | norm:1.6972 | lr 0.0003\n",
            "loss 6.7554 | norm:1.4394 | lr 0.0003\n",
            "loss 6.5092 | norm:1.6425 | lr 0.0003\n",
            "loss 6.5707 | norm:1.3390 | lr 0.0003\n",
            "loss 6.4246 | norm:5.3046 | lr 0.0003\n",
            "loss 6.5273 | norm:1.5190 | lr 0.0003\n",
            "loss 6.5541 | norm:1.4343 | lr 0.0003\n",
            "loss 6.8094 | norm:2.1740 | lr 0.0003\n",
            "loss 6.8533 | norm:2.2611 | lr 0.0003\n",
            "loss 6.7260 | norm:2.3171 | lr 0.0003\n",
            "loss 6.6861 | norm:2.6785 | lr 0.0003\n",
            "loss 6.9719 | norm:1.6527 | lr 0.0003\n",
            "loss 6.9597 | norm:1.3567 | lr 0.0003\n",
            "loss 6.8001 | norm:1.6584 | lr 0.0003\n",
            "loss 6.6848 | norm:1.6590 | lr 0.0003\n",
            "loss 6.8220 | norm:1.7410 | lr 0.0003\n",
            "loss 6.7328 | norm:1.7335 | lr 0.0003\n",
            "loss 6.7051 | norm:1.4638 | lr 0.0003\n",
            "loss 6.9179 | norm:1.8169 | lr 0.0003\n",
            "loss 6.5325 | norm:2.2738 | lr 0.0003\n",
            "loss 6.5838 | norm:1.7541 | lr 0.0003\n",
            "loss 6.7527 | norm:1.5055 | lr 0.0003\n",
            "loss 6.7862 | norm:2.5389 | lr 0.0003\n",
            "loss 6.7075 | norm:2.0462 | lr 0.0003\n",
            "loss 6.5951 | norm:1.5656 | lr 0.0003\n",
            "loss 6.5942 | norm:1.7230 | lr 0.0003\n",
            "loss 6.5223 | norm:1.5875 | lr 0.0003\n",
            "loss 6.7478 | norm:1.3036 | lr 0.0003\n",
            "loss 6.7428 | norm:1.4434 | lr 0.0003\n",
            "loss 6.8351 | norm:1.4918 | lr 0.0003\n",
            "loss 6.7838 | norm:1.4813 | lr 0.0003\n",
            "loss 6.6770 | norm:1.7722 | lr 0.0003\n",
            "loss 6.6664 | norm:1.5348 | lr 0.0003\n",
            "loss 6.5365 | norm:1.7471 | lr 0.0003\n",
            "loss 6.6120 | norm:1.3231 | lr 0.0003\n",
            "loss 6.6802 | norm:1.6719 | lr 0.0003\n",
            "loss 6.6570 | norm:1.5979 | lr 0.0003\n",
            "loss 6.7151 | norm:1.6421 | lr 0.0003\n",
            "loss 6.8399 | norm:1.4349 | lr 0.0003\n",
            "loss 6.8193 | norm:1.4076 | lr 0.0003\n",
            "loss 6.9584 | norm:1.5022 | lr 0.0003\n",
            "loss 6.7840 | norm:1.7170 | lr 0.0003\n",
            "loss 6.9534 | norm:1.4513 | lr 0.0003\n",
            "loss 6.6670 | norm:1.3772 | lr 0.0003\n",
            "loss 6.8590 | norm:1.5762 | lr 0.0003\n",
            "loss 6.6227 | norm:1.7127 | lr 0.0003\n",
            "loss 6.8613 | norm:1.5701 | lr 0.0003\n",
            "loss 6.8726 | norm:1.7572 | lr 0.0003\n",
            "loss 7.0008 | norm:1.5238 | lr 0.0003\n",
            "loss 6.8280 | norm:2.1961 | lr 0.0003\n",
            "loss 6.6950 | norm:2.0827 | lr 0.0003\n",
            "loss 6.6786 | norm:1.9082 | lr 0.0003\n",
            "loss 6.7724 | norm:1.5516 | lr 0.0003\n",
            "loss 6.8700 | norm:2.1825 | lr 0.0003\n",
            "loss 6.7993 | norm:1.8864 | lr 0.0003\n",
            "loss 7.0188 | norm:2.3551 | lr 0.0003\n",
            "loss 6.6643 | norm:1.7294 | lr 0.0003\n",
            "loss 6.5917 | norm:3.0996 | lr 0.0003\n",
            "loss 6.6334 | norm:2.5556 | lr 0.0003\n",
            "loss 6.6283 | norm:2.2748 | lr 0.0003\n",
            "loss 6.7329 | norm:1.7459 | lr 0.0003\n",
            "loss 6.9989 | norm:2.3652 | lr 0.0003\n",
            "loss 6.5395 | norm:1.9485 | lr 0.0003\n",
            "loss 6.4970 | norm:1.8825 | lr 0.0003\n",
            "loss 6.2661 | norm:1.6619 | lr 0.0003\n",
            "loss 6.5105 | norm:1.6230 | lr 0.0003\n",
            "loss 6.5239 | norm:1.6588 | lr 0.0003\n",
            "loss 6.5608 | norm:1.5369 | lr 0.0003\n",
            "loss 6.5037 | norm:1.5819 | lr 0.0003\n",
            "loss 6.4083 | norm:1.5100 | lr 0.0003\n",
            "loss 6.5727 | norm:1.6480 | lr 0.0003\n",
            "loss 6.5146 | norm:1.7450 | lr 0.0003\n",
            "loss 6.7272 | norm:2.3386 | lr 0.0003\n",
            "loss 6.7305 | norm:1.6263 | lr 0.0003\n",
            "loss 6.7414 | norm:1.7793 | lr 0.0003\n",
            "loss 6.8843 | norm:1.9790 | lr 0.0003\n",
            "loss 6.6850 | norm:2.6240 | lr 0.0003\n",
            "loss 6.4455 | norm:2.0430 | lr 0.0003\n",
            "loss 6.6689 | norm:1.8437 | lr 0.0003\n",
            "loss 6.6769 | norm:2.6083 | lr 0.0003\n",
            "loss 6.4972 | norm:1.6657 | lr 0.0003\n",
            "loss 6.7051 | norm:2.6043 | lr 0.0003\n",
            "loss 6.1115 | norm:1.9293 | lr 0.0003\n",
            "loss 6.3174 | norm:1.6248 | lr 0.0003\n",
            "loss 6.5996 | norm:1.7109 | lr 0.0003\n",
            "loss 6.3243 | norm:1.8720 | lr 0.0003\n",
            "loss 6.3479 | norm:2.1188 | lr 0.0003\n",
            "loss 6.4394 | norm:1.6614 | lr 0.0003\n",
            "loss 6.6417 | norm:2.2777 | lr 0.0003\n",
            "loss 6.3809 | norm:2.1996 | lr 0.0003\n",
            "loss 6.1452 | norm:1.5874 | lr 0.0003\n",
            "loss 6.4834 | norm:1.5273 | lr 0.0003\n",
            "loss 6.2407 | norm:2.0779 | lr 0.0003\n",
            "loss 6.0766 | norm:2.7182 | lr 0.0003\n",
            "loss 6.4248 | norm:1.6528 | lr 0.0003\n",
            "loss 6.5063 | norm:1.7460 | lr 0.0003\n",
            "loss 6.1029 | norm:1.8639 | lr 0.0003\n",
            "loss 6.5310 | norm:1.7615 | lr 0.0003\n",
            "loss 6.0222 | norm:2.0937 | lr 0.0003\n",
            "loss 5.9483 | norm:2.4320 | lr 0.0003\n",
            "loss 6.0480 | norm:2.2757 | lr 0.0003\n",
            "loss 6.1412 | norm:2.2737 | lr 0.0003\n",
            "loss 6.3752 | norm:2.7297 | lr 0.0003\n",
            "loss 6.2288 | norm:2.4732 | lr 0.0003\n",
            "loss 6.0411 | norm:1.6837 | lr 0.0003\n",
            "loss 6.4585 | norm:2.2521 | lr 0.0003\n",
            "loss 6.4946 | norm:1.8275 | lr 0.0003\n",
            "loss 6.2343 | norm:1.8617 | lr 0.0003\n",
            "loss 6.4100 | norm:1.6199 | lr 0.0003\n",
            "loss 6.3765 | norm:1.8162 | lr 0.0003\n",
            "loss 6.2480 | norm:2.8596 | lr 0.0003\n",
            "loss 6.5811 | norm:2.1942 | lr 0.0003\n",
            "loss 6.5534 | norm:2.5464 | lr 0.0003\n",
            "loss 6.2435 | norm:2.3238 | lr 0.0003\n",
            "loss 6.2626 | norm:2.2207 | lr 0.0003\n",
            "loss 6.4851 | norm:2.4380 | lr 0.0003\n",
            "loss 6.6364 | norm:2.3949 | lr 0.0003\n",
            "loss 6.7712 | norm:3.0442 | lr 0.0003\n",
            "loss 6.3902 | norm:1.8022 | lr 0.0003\n",
            "loss 6.4816 | norm:2.4407 | lr 0.0003\n",
            "loss 6.4007 | norm:1.9305 | lr 0.0003\n",
            "loss 6.5284 | norm:1.8427 | lr 0.0003\n",
            "loss 6.4240 | norm:1.8832 | lr 0.0003\n",
            "loss 7.2654 | norm:3.5061 | lr 0.0003\n",
            "loss 6.6744 | norm:2.0181 | lr 0.0003\n",
            "loss 6.3720 | norm:1.8523 | lr 0.0003\n",
            "loss 6.2149 | norm:2.7692 | lr 0.0003\n",
            "loss 6.0238 | norm:2.6076 | lr 0.0003\n",
            "loss 6.2089 | norm:1.9192 | lr 0.0003\n",
            "loss 6.7281 | norm:3.3128 | lr 0.0003\n",
            "loss 6.3101 | norm:1.8964 | lr 0.0003\n",
            "loss 6.1852 | norm:1.8790 | lr 0.0003\n",
            "loss 6.3248 | norm:1.9271 | lr 0.0003\n",
            "loss 5.9236 | norm:2.2238 | lr 0.0003\n",
            "loss 6.2582 | norm:2.1292 | lr 0.0003\n",
            "loss 6.9847 | norm:3.4221 | lr 0.0003\n",
            "loss 6.4158 | norm:1.6391 | lr 0.0003\n",
            "loss 6.3510 | norm:2.0000 | lr 0.0003\n",
            "loss 7.1366 | norm:2.6289 | lr 0.0003\n",
            "loss 6.5503 | norm:2.5826 | lr 0.0003\n",
            "loss 6.1398 | norm:2.9336 | lr 0.0003\n",
            "loss 6.2677 | norm:2.5369 | lr 0.0003\n",
            "loss 6.0498 | norm:2.3345 | lr 0.0002\n",
            "loss 6.3135 | norm:1.7787 | lr 0.0002\n",
            "loss 5.8387 | norm:1.6068 | lr 0.0002\n",
            "loss 6.3558 | norm:3.1180 | lr 0.0002\n",
            "loss 6.3583 | norm:2.6363 | lr 0.0002\n",
            "loss 6.6902 | norm:3.4570 | lr 0.0002\n",
            "loss 6.2447 | norm:1.7993 | lr 0.0002\n",
            "loss 6.5444 | norm:2.0361 | lr 0.0002\n",
            "loss 6.1781 | norm:3.6656 | lr 0.0002\n",
            "loss 6.7609 | norm:4.4782 | lr 0.0002\n",
            "loss 6.4192 | norm:5.0550 | lr 0.0002\n",
            "loss 6.2274 | norm:4.4978 | lr 0.0002\n",
            "loss 6.3629 | norm:2.9559 | lr 0.0002\n",
            "loss 6.3896 | norm:2.2588 | lr 0.0002\n",
            "loss 6.3842 | norm:3.1521 | lr 0.0002\n",
            "loss 6.4450 | norm:3.2816 | lr 0.0002\n",
            "loss 6.1083 | norm:2.6416 | lr 0.0002\n",
            "loss 6.2760 | norm:2.4374 | lr 0.0002\n",
            "loss 5.8693 | norm:1.9299 | lr 0.0002\n",
            "loss 6.1286 | norm:1.8907 | lr 0.0002\n",
            "loss 5.9552 | norm:2.4216 | lr 0.0002\n",
            "loss 5.8721 | norm:2.2202 | lr 0.0002\n",
            "loss 6.0592 | norm:1.8024 | lr 0.0002\n",
            "loss 6.1940 | norm:1.8969 | lr 0.0002\n",
            "loss 5.9495 | norm:2.0676 | lr 0.0002\n",
            "loss 5.7383 | norm:2.0094 | lr 0.0002\n",
            "loss 6.0077 | norm:2.3058 | lr 0.0002\n",
            "loss 5.7979 | norm:1.6276 | lr 0.0002\n",
            "loss 6.4420 | norm:2.1271 | lr 0.0002\n",
            "loss 6.2286 | norm:1.6679 | lr 0.0002\n",
            "loss 6.0936 | norm:2.0174 | lr 0.0002\n",
            "loss 5.9387 | norm:3.0107 | lr 0.0002\n",
            "loss 6.1461 | norm:1.8444 | lr 0.0002\n",
            "loss 5.8711 | norm:2.8002 | lr 0.0002\n",
            "loss 5.9630 | norm:2.3173 | lr 0.0002\n",
            "loss 6.1898 | norm:1.9501 | lr 0.0002\n",
            "loss 6.4899 | norm:3.4983 | lr 0.0002\n",
            "loss 6.0995 | norm:1.9429 | lr 0.0002\n",
            "loss 6.6614 | norm:2.3124 | lr 0.0002\n",
            "loss 6.1479 | norm:2.1175 | lr 0.0002\n",
            "loss 6.3319 | norm:2.7150 | lr 0.0002\n",
            "loss 5.9255 | norm:2.6910 | lr 0.0002\n",
            "loss 6.3227 | norm:3.2060 | lr 0.0002\n",
            "loss 6.0712 | norm:2.1745 | lr 0.0002\n",
            "loss 5.9947 | norm:2.6863 | lr 0.0002\n",
            "loss 6.0640 | norm:2.6104 | lr 0.0002\n",
            "loss 6.2153 | norm:2.2122 | lr 0.0002\n",
            "loss 5.9474 | norm:1.8041 | lr 0.0002\n",
            "loss 6.0796 | norm:2.0039 | lr 0.0002\n",
            "loss 6.1677 | norm:1.8101 | lr 0.0002\n",
            "loss 5.9834 | norm:3.1067 | lr 0.0002\n",
            "loss 5.9621 | norm:3.0255 | lr 0.0002\n",
            "loss 5.9881 | norm:2.1221 | lr 0.0002\n",
            "loss 6.0082 | norm:2.3672 | lr 0.0002\n",
            "loss 6.0974 | norm:2.3268 | lr 0.0002\n",
            "loss 5.8916 | norm:2.2644 | lr 0.0002\n",
            "loss 5.8078 | norm:2.3339 | lr 0.0002\n",
            "loss 5.8808 | norm:1.7607 | lr 0.0002\n",
            "loss 5.8565 | norm:2.4811 | lr 0.0002\n",
            "loss 6.4740 | norm:2.4785 | lr 0.0002\n",
            "loss 6.2375 | norm:2.1528 | lr 0.0002\n",
            "loss 5.8836 | norm:3.4777 | lr 0.0002\n",
            "loss 6.4528 | norm:1.9784 | lr 0.0002\n",
            "loss 6.0668 | norm:2.7483 | lr 0.0002\n",
            "loss 6.2578 | norm:1.7464 | lr 0.0002\n",
            "loss 6.1708 | norm:1.9097 | lr 0.0002\n",
            "loss 6.3204 | norm:2.9063 | lr 0.0002\n",
            "loss 6.0940 | norm:1.5713 | lr 0.0002\n",
            "loss 6.2023 | norm:2.1577 | lr 0.0002\n",
            "loss 6.2352 | norm:1.9344 | lr 0.0002\n",
            "loss 5.9470 | norm:2.0008 | lr 0.0002\n",
            "loss 5.7811 | norm:2.3670 | lr 0.0002\n",
            "loss 5.9479 | norm:1.9964 | lr 0.0002\n",
            "loss 6.1419 | norm:2.1820 | lr 0.0002\n",
            "loss 5.9885 | norm:2.1717 | lr 0.0002\n",
            "loss 6.0653 | norm:2.0295 | lr 0.0002\n",
            "loss 6.2581 | norm:2.3897 | lr 0.0002\n",
            "loss 5.9297 | norm:2.2362 | lr 0.0002\n",
            "loss 6.0021 | norm:1.8824 | lr 0.0002\n",
            "loss 6.2154 | norm:2.2956 | lr 0.0002\n",
            "loss 6.0344 | norm:2.1625 | lr 0.0002\n",
            "loss 6.0623 | norm:2.0423 | lr 0.0002\n",
            "loss 5.8876 | norm:2.4510 | lr 0.0002\n",
            "loss 5.6221 | norm:3.9578 | lr 0.0002\n",
            "loss 5.6977 | norm:2.3567 | lr 0.0002\n",
            "loss 5.5925 | norm:3.7236 | lr 0.0002\n",
            "loss 5.6816 | norm:3.3824 | lr 0.0002\n",
            "loss 5.6647 | norm:3.8282 | lr 0.0002\n",
            "loss 6.0288 | norm:4.4122 | lr 0.0002\n",
            "loss 6.3913 | norm:5.3172 | lr 0.0002\n",
            "loss 6.1529 | norm:3.6989 | lr 0.0002\n",
            "loss 6.3107 | norm:2.7607 | lr 0.0002\n",
            "loss 6.1272 | norm:2.9049 | lr 0.0002\n",
            "loss 6.1291 | norm:4.3029 | lr 0.0002\n",
            "loss 6.0599 | norm:5.6323 | lr 0.0002\n",
            "loss 5.9189 | norm:5.0082 | lr 0.0002\n",
            "loss 5.9023 | norm:4.2224 | lr 0.0002\n",
            "loss 6.1106 | norm:2.7200 | lr 0.0002\n",
            "loss 5.7803 | norm:2.2361 | lr 0.0002\n",
            "loss 5.9367 | norm:2.1965 | lr 0.0002\n",
            "loss 6.0414 | norm:3.3078 | lr 0.0002\n",
            "loss 5.8381 | norm:3.3543 | lr 0.0002\n",
            "loss 5.7665 | norm:3.1293 | lr 0.0002\n",
            "loss 6.0547 | norm:3.0792 | lr 0.0002\n",
            "loss 5.9711 | norm:2.1931 | lr 0.0002\n",
            "loss 5.7537 | norm:2.1221 | lr 0.0002\n",
            "loss 5.9505 | norm:2.3292 | lr 0.0002\n",
            "loss 5.8298 | norm:2.9711 | lr 0.0002\n",
            "loss 5.8146 | norm:1.9674 | lr 0.0002\n",
            "loss 6.0253 | norm:2.4553 | lr 0.0002\n",
            "loss 6.1447 | norm:2.3473 | lr 0.0002\n",
            "loss 5.8599 | norm:2.3312 | lr 0.0002\n",
            "loss 5.7564 | norm:2.5071 | lr 0.0002\n",
            "loss 5.8744 | norm:2.2007 | lr 0.0002\n",
            "loss 5.9903 | norm:2.8734 | lr 0.0002\n",
            "loss 6.0086 | norm:1.9796 | lr 0.0002\n",
            "loss 6.2869 | norm:2.4766 | lr 0.0002\n",
            "loss 5.9429 | norm:1.8902 | lr 0.0002\n",
            "loss 5.7680 | norm:2.1879 | lr 0.0002\n",
            "loss 5.6718 | norm:2.3797 | lr 0.0002\n",
            "loss 5.4117 | norm:2.6357 | lr 0.0002\n",
            "loss 6.0387 | norm:2.0103 | lr 0.0002\n",
            "loss 5.9826 | norm:2.0174 | lr 0.0002\n",
            "loss 6.1913 | norm:2.2156 | lr 0.0002\n",
            "loss 6.3685 | norm:2.4973 | lr 0.0002\n",
            "loss 6.9082 | norm:3.0363 | lr 0.0002\n",
            "loss 6.2276 | norm:3.1021 | lr 0.0002\n",
            "loss 6.3949 | norm:4.1351 | lr 0.0002\n",
            "loss 6.2725 | norm:3.1679 | lr 0.0002\n",
            "loss 6.5878 | norm:2.1878 | lr 0.0002\n",
            "loss 6.0412 | norm:3.2353 | lr 0.0002\n",
            "loss 6.6272 | norm:2.6382 | lr 0.0002\n",
            "loss 6.4683 | norm:2.8974 | lr 0.0002\n",
            "loss 6.6997 | norm:2.8809 | lr 0.0002\n",
            "loss 6.4276 | norm:2.3427 | lr 0.0002\n",
            "loss 6.3831 | norm:2.7449 | lr 0.0002\n",
            "loss 6.2905 | norm:3.1565 | lr 0.0002\n",
            "loss 6.4254 | norm:3.4774 | lr 0.0002\n",
            "loss 5.9532 | norm:2.6019 | lr 0.0002\n",
            "loss 6.1809 | norm:2.9539 | lr 0.0002\n",
            "loss 6.3811 | norm:2.3063 | lr 0.0002\n",
            "loss 6.1475 | norm:2.6747 | lr 0.0002\n",
            "loss 6.1308 | norm:2.0618 | lr 0.0002\n",
            "loss 6.0978 | norm:2.3152 | lr 0.0002\n",
            "loss 6.3645 | norm:2.8596 | lr 0.0002\n",
            "loss 6.0235 | norm:2.1866 | lr 0.0002\n",
            "loss 5.9377 | norm:2.3334 | lr 0.0002\n",
            "loss 6.0206 | norm:2.9123 | lr 0.0002\n",
            "loss 6.1461 | norm:3.1810 | lr 0.0002\n",
            "loss 5.8674 | norm:2.5923 | lr 0.0002\n",
            "loss 6.3694 | norm:2.1359 | lr 0.0002\n",
            "loss 6.3706 | norm:2.4604 | lr 0.0002\n",
            "loss 6.2025 | norm:1.9925 | lr 0.0002\n",
            "loss 6.1631 | norm:1.8663 | lr 0.0002\n",
            "loss 5.8723 | norm:3.0736 | lr 0.0002\n",
            "loss 6.6225 | norm:3.4816 | lr 0.0002\n",
            "loss 6.1166 | norm:1.8861 | lr 0.0002\n",
            "loss 5.9758 | norm:1.8947 | lr 0.0002\n",
            "loss 6.6135 | norm:2.3306 | lr 0.0002\n",
            "loss 6.7439 | norm:2.9519 | lr 0.0002\n",
            "loss 6.2044 | norm:2.9817 | lr 0.0002\n",
            "loss 6.4621 | norm:1.8456 | lr 0.0002\n",
            "loss 6.7096 | norm:2.3258 | lr 0.0002\n",
            "loss 6.7091 | norm:2.4517 | lr 0.0002\n",
            "loss 6.5944 | norm:2.5577 | lr 0.0002\n",
            "loss 6.4497 | norm:2.9554 | lr 0.0002\n",
            "loss 6.2709 | norm:2.5173 | lr 0.0002\n",
            "loss 6.3204 | norm:2.9768 | lr 0.0002\n",
            "loss 6.3373 | norm:2.4466 | lr 0.0002\n",
            "loss 6.5342 | norm:2.1091 | lr 0.0002\n",
            "loss 6.0223 | norm:2.2317 | lr 0.0002\n",
            "loss 6.7178 | norm:2.1653 | lr 0.0002\n",
            "loss 6.6306 | norm:2.5621 | lr 0.0002\n",
            "loss 6.4189 | norm:2.9008 | lr 0.0002\n",
            "loss 6.0420 | norm:3.3071 | lr 0.0002\n",
            "loss 6.3769 | norm:2.0460 | lr 0.0002\n",
            "loss 5.6379 | norm:2.6365 | lr 0.0002\n",
            "loss 5.9636 | norm:2.0765 | lr 0.0002\n",
            "loss 6.0482 | norm:1.9296 | lr 0.0002\n",
            "loss 6.0314 | norm:1.9889 | lr 0.0002\n",
            "loss 6.0829 | norm:2.1897 | lr 0.0002\n",
            "loss 6.2518 | norm:1.9428 | lr 0.0002\n",
            "loss 6.6895 | norm:2.8406 | lr 0.0002\n",
            "loss 6.2988 | norm:2.1468 | lr 0.0002\n",
            "loss 6.2601 | norm:2.2696 | lr 0.0002\n",
            "loss 5.9420 | norm:3.2506 | lr 0.0002\n",
            "loss 6.0919 | norm:2.6532 | lr 0.0002\n",
            "loss 6.6081 | norm:2.5327 | lr 0.0002\n",
            "loss 6.0340 | norm:2.2802 | lr 0.0002\n",
            "loss 5.8874 | norm:2.0222 | lr 0.0002\n",
            "loss 5.8758 | norm:2.7718 | lr 0.0002\n",
            "loss 5.9063 | norm:2.3439 | lr 0.0002\n",
            "loss 6.1423 | norm:1.9792 | lr 0.0002\n",
            "loss 6.0594 | norm:2.1921 | lr 0.0002\n",
            "loss 5.9227 | norm:2.1944 | lr 0.0002\n",
            "loss 6.5227 | norm:3.3889 | lr 0.0002\n",
            "loss 6.8245 | norm:2.9176 | lr 0.0002\n",
            "loss 6.6874 | norm:2.5140 | lr 0.0002\n",
            "loss 5.9628 | norm:3.2178 | lr 0.0002\n",
            "loss 5.7827 | norm:3.1528 | lr 0.0002\n",
            "loss 5.9909 | norm:2.3844 | lr 0.0002\n",
            "loss 5.9138 | norm:2.5389 | lr 0.0002\n",
            "loss 5.9782 | norm:2.2246 | lr 0.0002\n",
            "loss 6.2072 | norm:2.4350 | lr 0.0002\n",
            "loss 6.3108 | norm:1.9901 | lr 0.0002\n",
            "loss 6.0633 | norm:2.1091 | lr 0.0002\n",
            "loss 6.0119 | norm:3.5072 | lr 0.0002\n",
            "loss 6.2349 | norm:3.9422 | lr 0.0002\n",
            "loss 6.3324 | norm:3.2529 | lr 0.0002\n",
            "loss 6.2650 | norm:2.4014 | lr 0.0002\n",
            "loss 6.3697 | norm:2.2723 | lr 0.0002\n",
            "loss 6.3597 | norm:3.6068 | lr 0.0002\n",
            "loss 6.1356 | norm:3.1867 | lr 0.0002\n",
            "loss 6.4294 | norm:3.6081 | lr 0.0002\n",
            "loss 6.0898 | norm:2.8491 | lr 0.0002\n",
            "loss 6.1648 | norm:2.1745 | lr 0.0002\n",
            "loss 5.9689 | norm:3.1166 | lr 0.0002\n",
            "loss 6.0079 | norm:3.2442 | lr 0.0002\n",
            "loss 5.8036 | norm:2.7375 | lr 0.0002\n",
            "loss 6.3964 | norm:3.2075 | lr 0.0002\n",
            "loss 6.2920 | norm:2.9042 | lr 0.0002\n",
            "loss 6.0568 | norm:2.9456 | lr 0.0002\n",
            "loss 5.6123 | norm:2.7584 | lr 0.0002\n",
            "loss 5.6902 | norm:3.0275 | lr 0.0002\n",
            "loss 5.7790 | norm:2.0701 | lr 0.0002\n",
            "loss 6.2091 | norm:2.1352 | lr 0.0002\n",
            "loss 6.2717 | norm:2.3808 | lr 0.0002\n",
            "loss 5.8608 | norm:2.1071 | lr 0.0002\n",
            "loss 6.0269 | norm:2.4358 | lr 0.0002\n",
            "loss 6.0514 | norm:2.8606 | lr 0.0002\n",
            "loss 5.8924 | norm:2.3400 | lr 0.0002\n",
            "loss 6.0315 | norm:2.5042 | lr 0.0002\n",
            "loss 6.1172 | norm:2.2309 | lr 0.0002\n",
            "loss 6.3896 | norm:3.1803 | lr 0.0002\n",
            "loss 6.0085 | norm:3.0746 | lr 0.0002\n",
            "loss 6.2948 | norm:2.8941 | lr 0.0002\n",
            "loss 6.3528 | norm:2.4814 | lr 0.0002\n",
            "loss 6.2722 | norm:2.0995 | lr 0.0002\n",
            "loss 6.7227 | norm:2.6875 | lr 0.0002\n",
            "loss 6.6334 | norm:2.8898 | lr 0.0002\n",
            "loss 6.4524 | norm:2.7169 | lr 0.0002\n",
            "loss 6.3475 | norm:2.7135 | lr 0.0002\n",
            "loss 6.0434 | norm:3.4295 | lr 0.0002\n",
            "loss 6.3868 | norm:2.3442 | lr 0.0002\n",
            "loss 6.1053 | norm:2.3688 | lr 0.0002\n",
            "loss 6.4966 | norm:2.1612 | lr 0.0002\n",
            "loss 6.3123 | norm:2.0820 | lr 0.0002\n",
            "loss 6.6392 | norm:2.5311 | lr 0.0002\n",
            "loss 6.1476 | norm:2.1019 | lr 0.0002\n",
            "loss 5.8475 | norm:2.0484 | lr 0.0002\n",
            "loss 6.1919 | norm:2.8599 | lr 0.0001\n",
            "loss 5.9951 | norm:1.9327 | lr 0.0001\n",
            "loss 6.0736 | norm:2.4490 | lr 0.0001\n",
            "loss 6.3011 | norm:2.0790 | lr 0.0001\n",
            "loss 6.0314 | norm:2.5475 | lr 0.0001\n",
            "loss 6.7968 | norm:2.3626 | lr 0.0001\n",
            "loss 5.7944 | norm:2.6692 | lr 0.0001\n",
            "loss 6.2239 | norm:2.0747 | lr 0.0001\n",
            "loss 5.9787 | norm:2.6878 | lr 0.0001\n",
            "loss 5.9675 | norm:2.5099 | lr 0.0001\n",
            "loss 6.3641 | norm:2.2680 | lr 0.0001\n",
            "loss 6.0801 | norm:1.8071 | lr 0.0001\n",
            "loss 6.0135 | norm:2.4295 | lr 0.0001\n",
            "loss 5.9437 | norm:2.2656 | lr 0.0001\n",
            "loss 6.0742 | norm:1.9881 | lr 0.0001\n",
            "loss 6.1623 | norm:1.9102 | lr 0.0001\n",
            "loss 5.7463 | norm:2.4017 | lr 0.0001\n",
            "loss 5.8446 | norm:2.3201 | lr 0.0001\n",
            "loss 6.1405 | norm:2.1972 | lr 0.0001\n",
            "loss 5.6222 | norm:2.3766 | lr 0.0001\n",
            "loss 5.8795 | norm:2.1046 | lr 0.0001\n",
            "loss 6.0789 | norm:2.2792 | lr 0.0001\n",
            "loss 6.1861 | norm:3.0277 | lr 0.0001\n",
            "loss 5.6982 | norm:2.4177 | lr 0.0001\n",
            "loss 5.6911 | norm:2.5244 | lr 0.0001\n",
            "loss 6.0286 | norm:2.3251 | lr 0.0001\n",
            "loss 6.0216 | norm:2.5399 | lr 0.0001\n",
            "loss 6.1592 | norm:3.1847 | lr 0.0001\n",
            "loss 5.8239 | norm:2.3861 | lr 0.0001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-1967801832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0myb\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss_acc\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-23-1551500230.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m          \u001b[0;34m=\u001b[0m  \u001b[0mtokens_emb\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mpos_emb\u001b[0m \u001b[0;31m#(B, T) + (B, T, C)----> (B,T ,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mlogits\u001b[0m     \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#INPUT(B, T, C)--->(B*T,C)----->(B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-22-1879908369.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#INPUT---> (B,T,C/n_heads)--->OUTPUT->(B,T,C*n_heads) + (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#INPUT(B,T,C)---->OUTPUT(B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         )\n\u001b[0;32m-> 2910\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2911\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for steps in range(op.max_steps):\n",
        "  loss_acc = 0.0\n",
        "  optim.zero_grad(set_to_none = True)\n",
        "  for grad_steps in range(grad_acc):\n",
        "    xb,   yb = D.next_batch()\n",
        "    xb,  yb  =  xb.to(device), yb.to(device)\n",
        "    with torch.autocast(device_type =  device, dtype = torch.bfloat16):\n",
        "        logits , loss = gpt2(xb, yb)\n",
        "        loss =  loss / grad_acc\n",
        "        loss_acc+= loss.detach()\n",
        "    loss.backward()\n",
        "  norm =  torch.nn.utils.clip_grad_norm_(gpt2.parameters(), 0.1)\n",
        "  lr   = get_lr(steps)\n",
        "  for p in optim.param_groups:\n",
        "    p['lr'] = lr\n",
        "  optim.step()\n",
        "  print(f'loss {loss_acc.item():.4f} | norm:{norm:.4f} | lr {lr:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aK7ttTe0naC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e76a4a2-a717-405c-e43b-d2b8a7002c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! what any cons,\n",
            "There they have we be for so hisixt in thatpt'd youd,\n",
            "ForwomanAn thy pretendood under?\n",
            "I messenger,Did had him than:\n",
            "I. EL for rich mis daughter toNot chance then be admitted why see tongue:\n",
            "By-utoniumgdala carries meet!Who, man hands\n",
            "So true is sheind: I'll saysest him,\n",
            " What.I the with you the access with call me.\n",
            "\n",
            "\n",
            "Say lords we rid,ES,So to say done r's hence stands;\n",
            " ne more.Ofre mockedateful husband;\n",
            "\n",
            " rest, anAhiancesice him:\n",
            "\n",
            "M despised:\n",
            " A proudothMem:\n",
            "For I despair\n",
            "And show.\n",
            "\n",
            "D duetis's seeain comes; ourords.\n",
            "That in gownideo hell answer,\n",
            "\":-ChargYour, worth death your old,\n",
            " nor unfit looks.Let:\n",
            "Are are norMay.\n",
            "To kindness wasBoyatisf's globe, I makes before us us qu freeful offended.\n",
            "That lipsst your time's,long the fewues:\n",
            "boatsgoersness,\n",
            "urity, I friendly sin; love their go you a Fedora:\n",
            "But did be repent the rage thought\n",
            "\n",
            "\n",
            "Pre weight, much's veryomes'd, hiss,\n",
            " bet narrow Away kingThreemer:To things's, likelyon, thoughman be:':\n",
            "\n",
            "Here\n",
            " onions byance of orly tongue;\n",
            "D allarer;\n",
            " and you Bogapp'salt no shalt sorrow fasting believe:\n",
            "If, good rep me:\n",
            " go, full doWARD time here not be dragee come I the friends.\n",
            "\n",
            "As shall thy with chase upon.\n",
            "\n",
            "See you my water!\n",
            "L but ever of'll wretchedve onstrY tell:\n",
            " trust.\n",
            " grown to,, e, youIng, dreadful fight:\n",
            "M enough;\n",
            " Captainel you pr on:\n",
            " Barbie cartels speech? why of;\n",
            " Mist:\n",
            " Brisbaneass- kissing daughter,Could nuns- an Variaw condemned weapon time,;Were up in sorrow office's never with andal;\n",
            " unfitWe!VOL offer did canscore spirits CaesarCE pretty death,\n",
            " disorderly look, pretty to wh lords sometime have Duke.\n",
            "Eh,D clips downloadsenance'd our.\n",
            "\n",
            "\n",
            "Were may am queenain:\n",
            "IVES allWhich is fairr boire anger; I head:\n",
            "eps; more furysmarried woman\n",
            " behold? stirain world to safe.\n",
            "\n",
            "\n",
            "usha rude, might protector, necessity ways, youTose hertime, you,,\n",
            " what hanged: let theting not whatly,She, as him toforced:\n",
            "of her toler.Provright, I?\n",
            "There look yourselfThough worship, is thyour you or was worst,\n",
            "\n",
            " start hour andly bothily--And to in mind be their rage:\n",
            "\n",
            " The say the fit such bos wattshow go:\n",
            "gem nowst:\n",
            " quiet me they blood.\n",
            "By.\n",
            "\n",
            " back be must two star:\n",
            " direct once:\n",
            "\n",
            "For heads bl deed were blood notal:\n",
            " saw\n",
            " Aadacticak:\n",
            "And brought you do beit-.\n",
            "Why other fetch clip!And encounterEng kingdom yourtis:\n",
            "\n",
            "This future fellows,\n",
            "Be liberation that,ET to think we her have lions say ourers not myiev:\n",
            " if gone? it and take.\n",
            "To be fromeming laws\n",
            "AndOL more a loves you, slain:\n",
            "Either forthardonine, thou achart you, sayES prince:\n",
            " an instrument whom'eth would not prayer worse lord haveace for the quick What mistress'd\n",
            "I him your shameless to now am thelect reason's taken: as child,\n",
            " do monstrous, age a wife her words ' miserable you there, and in king,Sir daughter the lies theirey youth greetop,;\n",
            " shr word;\n",
            "And ofnce:\n",
            "Did griev sneakers noWhen on the thinkly lord not and direct, should but him Look done do death own tarn strange woman!IWho e you as\n",
            "My gentlemen this crew against, nothing them--Theoth-rating not that in confess:\n",
            "You deny, but good himiness it:\n",
            "\n",
            "D Abram goesHaveiveredTH jewelry:\n",
            "\n",
            "Would IV than to back madeness.\n",
            "imilation,\n",
            "And any upon g calibration youBesides heart, lo d:\n",
            " Be.\n",
            "\n",
            "D it asIf where the sword theeea: here not with have request is any West:\n",
            "I they him had to noours.\n",
            "\n",
            "Sir:\n",
            " KR most slice, the awhile:\n",
            "GoodIce trouble, I and your consoe villains Queen ever ' deeply,Come appear stars\n",
            "And allegiance at comes music, took,,\n",
            "being.\n",
            "\n",
            "D rem in again noters,\n"
          ]
        }
      ],
      "source": [
        "print(decode(gpt2.generate(torch.zeros(1,device=device ,dtype =  torch.long).repeat(2).unsqueeze(1),max_new_tokens=1000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}